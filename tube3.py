# import requests
# import json
# import re
# import time
# from collections import Counter
# import sys
# from datetime import datetime
# from urllib.parse import quote
# import pandas as pd

# API_KEY = 'AIzaSyDbbn1H1GcuMKXMhhRl-wnld7KOz_JLTl4'
# BRAND_KEYWORDS = ['loreal', 'maybelline', 'lakme', 'mamaearth', 'nykaa', 'plum']

# # üî• CITY & STATE MAPPING
# INDIA_CITIES = {
#     'delhi': 'Delhi', 'mumbai': 'Maharashtra', 'bangalore': 'Karnataka', 
#     'pune': 'Maharashtra', 'kanpur': 'Uttar Pradesh', 'lucknow': 'Uttar Pradesh',
#     'noida': 'Uttar Pradesh', 'hyderabad': 'Telangana', 'chennai': 'Tamil Nadu',
#     'kolkata': 'West Bengal', 'ahmedabad': 'Gujarat', 'jaipur': 'Rajasthan'
# }

# def safe_api_call(url, retries=3):
#     """Safe API call"""
#     for attempt in range(retries):
#         try:
#             response = requests.get(url, timeout=10)
#             data = response.json()
#             if 'error' in data:
#                 print(f"‚ö†Ô∏è API Error: {data['error'].get('message', 'Unknown')}")
#                 time.sleep(2)
#                 continue
#             return data
#         except Exception as e:
#             print(f"‚ö†Ô∏è Request error: {e}")
#             time.sleep(1)
#     return None

# def search_keyword_multi_region(query):
#     """üî• WORLDWIDE + INDIA + STATES"""
#     print(f"üåç WORLDWIDE + INDIA ANALYSIS for '{query}'")
    
#     # üî• 1. WORLDWIDE (All regions)
#     worldwide_ids = set()
#     regions = ['US', 'GB', 'IN', 'CA', 'AU', 'DE', 'FR']
#     for region in regions:
#         print(f"   üåç Worldwide {region}...")
#         url = f"https://youtube.googleapis.com/youtube/v3/search?part=snippet&q={quote(query)}&type=video&maxResults=30&order=viewCount&regionCode={region}&key={API_KEY}"
#         data = safe_api_call(url)
#         if data and 'items' in data:
#             for item in data['items']:
#                 worldwide_ids.add(item['id']['videoId'])
#         time.sleep(0.3)
    
#     # üî• 2. INDIA SUBCATEGORIES
#     india_ids = set()
#     for mode in ['viewCount', 'relevance']:
#         print(f"   üáÆüá≥ India {mode}...")
#         url = f"https://youtube.googleapis.com/youtube/v3/search?part=snippet&q={quote(query)}&type=video&maxResults=50&order={mode}&regionCode=IN&key={API_KEY}"
#         data = safe_api_call(url)
#         if data and 'items' in data:
#             for item in data['items']:
#                 india_ids.add(item['id']['videoId'])
#         time.sleep(0.3)
    
#     return {
#         'worldwide': list(worldwide_ids),
#         'india': list(india_ids)
#     }

# def get_full_video_details(video_ids, region_label=""):
#     """üî• FULL DETAILS"""
#     all_videos = []
#     for i in range(0, len(video_ids), 50):
#         batch = video_ids[i:i+50]
#         print(f"üìä {region_label} Batch {i//50 + 1}...")
        
#         url = f"https://youtube.googleapis.com/youtube/v3/videos?part=snippet,statistics,contentDetails&id={','.join(batch)}&key={API_KEY}"
#         data = safe_api_call(url)
        
#         if data and 'items' in data:
#             for item in data['items']:
#                 try:
#                     video = {
#                         'Video_ID': item['id'],
#                         'Title': item['snippet'].get('title', ''),
#                         'Channel': item['snippet'].get('channelTitle', ''),
#                         'Description': item['snippet'].get('description', '')[:400],
#                         'Published': item['snippet'].get('publishedAt', ''),
#                         'Views': int(item['statistics'].get('viewCount', 0)),
#                         'Likes': int(item['statistics'].get('likeCount', 0)),
#                         'Comments': int(item['statistics'].get('commentCount', 0)),
#                         'Duration_Raw': item['contentDetails'].get('duration', 'PT0S'),
#                         'Video_URL': f"https://youtu.be/{item['id']}",
#                         'Region': region_label
#                     }
                    
#                     # Duration
#                     duration_match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', video['Duration_Raw'])
#                     if duration_match:
#                         h, m, s = duration_match.groups()
#                         total_sec = int(h or 0)*3600 + int(m or 0)*60 + int(s or 0)
#                         video['Duration'] = f"{total_sec//60}m {total_sec%60}s"
#                     else:
#                         video['Duration'] = '0s'
                    
#                     all_videos.append(video)
#                 except:
#                     continue
#         time.sleep(0.5)
    
#     return all_videos

# def detect_location_india(videos):
#     """üî• STATE & CITY DETECTION"""
#     for video in videos:
#         title_lower = video['Title'].lower()
#         desc_lower = video['Description'].lower()
#         text_lower = f"{title_lower} {desc_lower}"
        
#         # üî• City ‚Üí State mapping
#         detected_city = None
#         for city, state in INDIA_CITIES.items():
#             if city in text_lower:
#                 detected_city = city.title()
#                 video['City'] = detected_city
#                 video['State'] = state
#                 break
        
#         if not detected_city:
#             video['City'] = 'Other'
#             video['State'] = 'Other'
    
#     return videos

# def analyze_data(videos, region):
#     """üî• ANALYSIS"""
#     analysis = {
#         'hashtags': Counter(),
#         'hooks': Counter(),
#         'top_channels': Counter(),
#         'brands': Counter()
#     }
    
#     for video in videos:
#         text = f"{video['Title']} {video['Description']}"
#         hashtags = re.findall(r'#([a-zA-Z0-9_]+)', text, re.IGNORECASE)
#         analysis['hashtags'].update(hashtags)
        
#         hook = video['Title'][:35].strip('?!.')
#         analysis['hooks'][hook] += 1
        
#         analysis['top_channels'][video['Channel'][:30]] += 1
        
#         title_lower = video['Title'].lower()
#         for brand in BRAND_KEYWORDS:
#             if brand in title_lower:
#                 analysis['brands'][brand.title()] += 1
    
#     return analysis

# def save_complete_excel(worldwide_videos, india_videos, query):
#     """üî• 10+ SHEETS EXCEL"""
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = f"{query.upper().replace('-', '_')}_COMPLETE_{timestamp}.xlsx"
    
#     with pd.ExcelWriter(filename, engine='openpyxl') as writer:
#         # üî• WORLDWIDE
#         pd.DataFrame(worldwide_videos).to_excel(writer, 'WORLDWIDE_ALL', index=False)
#         pd.DataFrame(worldwide_videos).sort_values('Views', ascending=False).head(50).to_excel(writer, 'WORLDWIDE_TOP50', index=False)
        
#         # üî• INDIA
#         pd.DataFrame(india_videos).to_excel(writer, 'INDIA_ALL', index=False)
#         pd.DataFrame(india_videos).sort_values('Views', ascending=False).head(50).to_excel(writer, 'INDIA_TOP50', index=False)
        
#         # üî• STATES
#         india_with_location = detect_location_india(india_videos.copy())
#         state_df = pd.DataFrame(india_with_location)
#         state_df.to_excel(writer, 'INDIA_LOCATIONS', index=False)
        
#         # üî• STATE WISE SUMMARY
#         state_summary = state_df.groupby(['State', 'City']).agg({
#             'Views': 'sum', 'Likes': 'sum', 'Video_ID': 'count'
#         }).round(0).reset_index()
#         state_summary.columns = ['State', 'City', 'Total_Views', 'Total_Likes', 'Video_Count']
#         state_summary.to_excel(writer, 'STATE_SUMMARY', index=False)
        
#         # üî• WORLDWIDE HASHTAGS
#         ww_analysis = analyze_data(worldwide_videos, 'Worldwide')
#         hashtags_df = pd.DataFrame([{'Hashtag': k, 'Count': v} for k, v in ww_analysis['hashtags'].most_common(50)])
#         hashtags_df.to_excel(writer, 'WORLDWIDE_HASHTAGS', index=False)
        
#         # üî• WORLDWIDE HOOKS
#         hooks_df = pd.DataFrame([{'Hook': k[:50], 'Count': v} for k, v in ww_analysis['hooks'].most_common(30)])
#         hooks_df.to_excel(writer, 'WORLDWIDE_HOOKS', index=False)
    
#     print(f"\nüíæ ‚úÖ MASTER EXCEL: {filename}")
#     print("üìä 8 SHEETS: WORLDWIDE_ALL | INDIA_ALL | STATE_SUMMARY | TOP50s + More!")
#     return filename

# def print_summary(worldwide_videos, india_videos, query):
#     """üî• CONSOLE SUMMARY"""
#     print("\n" + "="*120)
#     print(f"üöÄ '{query.upper()}' - WORLDWIDE + INDIA + STATES")
#     print("="*120)
    
#     print(f"\nüåç WORLDWIDE: {len(worldwide_videos)} videos")
#     print(f"üëÄ Total Views: {sum(v['Views'] for v in worldwide_videos):,}")
    
#     print(f"\nüáÆüá≥ INDIA: {len(india_videos)} videos")
#     print(f"üëÄ Total Views: {sum(v['Views'] for v in india_videos):,}")
    
#     # üî• Top 5 Worldwide
#     top5_ww = sorted(worldwide_videos, key=lambda x: x['Views'], reverse=True)[:5]
#     print(f"\nüî• WORLDWIDE TOP 5:")
#     for i, v in enumerate(top5_ww, 1):
#         print(f"{i}. {v['Title'][:60]}... | üëÄ {v['Views']:,} | ‚ù§Ô∏è {v['Likes']:,}")

# def main():
#     """üî• ULTIMATE ANALYZER"""
#     print("üöÄ GLOBAL YOUTUBE ANALYZER v24.0")
#     print("=" * 100)
#     print("üåç WORLDWIDE + üáÆüá≥ INDIA STATES + üìä EXCEL!")
    
#     while True:
#         try:
#             print("\n" + "="*100)
#             query = input("üîç Enter keyword (quit): ").strip()
            
#             if query.lower() in ['quit', 'q', 'exit']:
#                 print("üëã COMPLETE!")
#                 break
            
#             if not query:
#                 continue
            
#             # üî• FULL PROCESSING
#             region_data = search_keyword_multi_region(query)
            
#             worldwide_videos = get_full_video_details(region_data['worldwide'], "Worldwide")
#             india_videos = get_full_video_details(region_data['india'], "India")
            
#             print_summary(worldwide_videos, india_videos, query)
#             excel_file = save_complete_excel(worldwide_videos, india_videos, query)
            
#             print(f"\n‚úÖ '{query}' ‚Üí {excel_file}")
#             print("üìä WORLDWIDE + INDIA STATES READY!")
            
#         except KeyboardInterrupt:
#             print("\nüëã Stopped!")
#             break
#         except Exception as e:
#             print(f"‚ùå Error: {e}")

# if __name__ == "__main__":
#     main()


# //////////////////////////////////////////////////////////////////////////////

# import streamlit as st
# import requests
# import json
# import re
# import time
# from collections import Counter
# from datetime import datetime
# from urllib.parse import quote
# import pandas as pd
# import plotly.express as px
# import io

# # Page config
# st.set_page_config(page_title="YouTube City Analyzer", layout="wide", page_icon="üì∫")

# BRAND_KEYWORDS = ['loreal', 'maybelline', 'lakme', 'mamaearth', 'nykaa', 'plum']

# INDIA_CITIES = {
#     'kanpur': 'Uttar Pradesh', 'lucknow': 'Uttar Pradesh', 'noida': 'Uttar Pradesh', 
#     'agra': 'Uttar Pradesh', 'varanasi': 'Uttar Pradesh', 'allahabad': 'Uttar Pradesh',
#     'ghaziabad': 'Uttar Pradesh', 'meerut': 'Uttar Pradesh', 'bareilly': 'Uttar Pradesh',
#     'mumbai': 'Maharashtra', 'pune': 'Maharashtra', 'nagpur': 'Maharashtra',
#     'bangalore': 'Karnataka', 'mysore': 'Karnataka', 'delhi': 'Delhi',
#     'chennai': 'Tamil Nadu', 'hyderabad': 'Telangana', 'kolkata': 'West Bengal',
#     'ahmedabad': 'Gujarat', 'jaipur': 'Rajasthan', 'kochi': 'Kerala'
# }

# def safe_api_call(url, api_key, retries=3):
#     """üî• Ultra-safe API call with full error handling"""
#     for attempt in range(retries):
#         try:
#             response = requests.get(url, timeout=20)
#             print(f"API Status: {response.status_code}")  # Debug
            
#             if response.status_code == 200:
#                 data = response.json()
#                 if 'error' not in data:
#                     return data
#                 else:
#                     print(f"API Error: {data['error']}")
#                     return None
#             elif response.status_code == 429:
#                 time.sleep(10)
#                 continue
#             else:
#                 print(f"HTTP Error: {response.status_code}")
#                 time.sleep(2)
#         except Exception as e:
#             print(f"Request Error: {e}")
#             time.sleep(2)
#     return None

# def test_api_key(api_key):
#     """üî• Simple test - just check if ANY response comes"""
#     url = f"https://youtube.googleapis.com/youtube/v3/search?q=test&maxResults=1&key={api_key}"
#     data = safe_api_call(url, api_key)
#     return data is not None

# def search_videos(query, api_key, max_results=20):
#     """üî• Simplified search - works with ANY valid key"""
#     video_ids = set()
    
#     # Simple worldwide search
#     url = f"https://youtube.googleapis.com/youtube/v3/search?part=snippet&q={quote(query)}&type=video&maxResults={max_results}&order=viewCount&key={api_key}"
#     data = safe_api_call(url, api_key)
    
#     if data and 'items' in data:
#         for item in data['items']:
#             video_ids.add(item['id']['videoId'])
    
#     # India focused
#     url_in = f"https://youtube.googleapis.com/youtube/v3/search?part=snippet&q={quote(query)}&type=video&maxResults={max_results}&regionCode=IN&order=viewCount&key={api_key}"
#     data_in = safe_api_call(url_in, api_key)
    
#     if data_in and 'items' in data_in:
#         for item in data_in['items']:
#             video_ids.add(item['id']['videoId'])
    
#     return list(video_ids)[:40]

# def get_video_details(video_ids, api_key):
#     """üî• Get video details in small batches"""
#     all_videos = []
    
#     for i in range(0, len(video_ids), 20):  # Smaller batches
#         batch = video_ids[i:i+20]
#         url = f"https://youtube.googleapis.com/youtube/v3/videos?part=snippet,statistics,contentDetails&id={','.join(batch)}&key={api_key}"
#         data = safe_api_call(url, api_key)
        
#         if data and 'items' in data:
#             for item in data['items']:
#                 try:
#                     video = {
#                         'Video_ID': item['id'],
#                         'Title': item['snippet'].get('title', '')[:100],
#                         'Channel': item['snippet'].get('channelTitle', ''),
#                         'Description': item['snippet'].get('description', '')[:300],
#                         'Published': item['snippet'].get('publishedAt', ''),
#                         'Views': int(item['statistics'].get('viewCount', 0) or 0),
#                         'Likes': int(item['statistics'].get('likeCount', 0) or 0),
#                         'Comments': int(item['statistics'].get('commentCount', 0) or 0),
#                         'Duration': item['contentDetails'].get('duration', 'PT0S'),
#                         'Video_URL': f"https://youtu.be/{item['id']}",
#                         'Region': 'Mixed',
#                         'City': 'Other',
#                         'State': 'Other'
#                     }
                    
#                     # Duration
#                     duration_match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', video['Duration'])
#                     if duration_match:
#                         h, m, s = duration_match.groups()
#                         total_sec = (int(h or 0)*3600 + int(m or 0)*60 + int(s or 0))
#                         video['Duration_Formatted'] = f"{total_sec//60}m {total_sec%60:02d}s"
#                     else:
#                         video['Duration_Formatted'] = '0s'
                    
#                     all_videos.append(video)
#                     time.sleep(0.5)
#                 except Exception as e:
#                     print(f"Video parse error: {e}")
#                     continue
    
#     return all_videos

# def detect_locations(videos):
#     """üî• City/State detection"""
#     city_counter = Counter()
#     state_counter = Counter()
    
#     for video in videos:
#         text = (video['Title'] + ' ' + video['Description']).lower()
#         for city, state in INDIA_CITIES.items():
#             if city in text:
#                 video['City'] = city.title()
#                 video['State'] = state
#                 city_counter[video['City']] += 1
#                 state_counter[state] += 1
#                 break
#         else:
#             video['City'] = 'Other'
#             video['State'] = 'Other'
    
#     return videos, city_counter, state_counter

# def create_excel_bytes(worldwide_videos, india_videos, city_counter, state_counter, query):
#     """üî• Create Excel in memory - NO temp files"""
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = f"{query.upper().replace(' ', '_')}_ANALYSIS_{timestamp}.xlsx"
    
#     output = io.BytesIO()
#     with pd.ExcelWriter(output, engine='openpyxl') as writer:
#         # All data
#         pd.DataFrame(worldwide_videos).to_excel(writer, 'ALL_VIDEOS', index=False)
        
#         # Top videos
#         top_all = sorted(worldwide_videos + india_videos, key=lambda x: x['Views'], reverse=True)[:50]
#         pd.DataFrame(top_all).to_excel(writer, 'TOP_50_VIDEOS', index=False)
        
#         # Cities
#         city_df = pd.DataFrame(city_counter.most_common(20), columns=['City', 'Videos'])
#         city_df.to_excel(writer, 'CITY_RANKING', index=False)
        
#         # States
#         state_df = pd.DataFrame(state_counter.most_common(15), columns=['State', 'Videos'])
#         state_df.to_excel(writer, 'STATE_RANKING', index=False)
    
#     output.seek(0)
#     return output.getvalue(), filename

# # üî• MAIN APP
# st.title("üöÄ YouTube City Analyzer v26.0 - PERFECT!")
# st.markdown("***‚úÖ Works with ANY valid API key | No errors | Full dashboard***")

# # üî• Sidebar
# st.sidebar.header("üîë API Setup")
# api_key = st.sidebar.text_input("Your YouTube API Key:", type="password", 
#                                placeholder="AIzaSyC... (60 characters)")

# query = st.sidebar.text_input("üîç Keyword:", value="lip balm")
# max_results = st.sidebar.slider("Max Videos/Region", 10, 30, 20)

# # üî• Test API
# if st.sidebar.button("üß™ Test API Key", type="secondary"):
#     if api_key:
#         if test_api_key(api_key):
#             st.sidebar.success("‚úÖ API KEY PERFECT! üéâ")
#             st.sidebar.markdown("**Ready for analysis!**")
#         else:
#             st.sidebar.error("‚ùå API Key failed")
#             st.sidebar.info("1. Check key copied correctly\n2. Enable YouTube Data API v3\n3. Check quota")
#     else:
#         st.sidebar.warning("üëà Enter API key first")

# # üî• ANALYZE BUTTON
# if st.sidebar.button("üöÄ ANALYZE NOW", type="primary", disabled=not api_key):
#     if test_api_key(api_key):
#         with st.spinner("üîÑ Fetching YouTube data..."):
#             # üî• Get data
#             video_ids = search_videos(query, api_key, max_results)
#             all_videos = get_video_details(video_ids, api_key)
            
#             if all_videos:
#                 analyzed_videos, city_counter, state_counter = detect_locations(all_videos)
                
#                 # üî• DASHBOARD
#                 st.header("üìä LIVE RESULTS")
                
#                 # Metrics
#                 col1, col2, col3, col4 = st.columns(4)
#                 col1.metric("üì∫ Total Videos", len(all_videos))
#                 col2.metric("üëÄ Total Views", f"{sum(v['Views'] for v in all_videos):,}")
#                 col3.metric("üèôÔ∏è Cities Found", len([c for c in city_counter if c != 'Other']))
#                 col4.metric("‚ù§Ô∏è Total Likes", f"{sum(v['Likes'] for v in all_videos):,}")
                
#                 # üî• Top Videos
#                 st.subheader("üî• TOP VIDEOS")
#                 top_videos = sorted(all_videos, key=lambda x: x['Views'], reverse=True)[:20]
#                 df_top = pd.DataFrame(top_videos)[['Title', 'Channel', 'Views', 'Likes', 'Duration_Formatted', 'Video_URL']]
#                 st.dataframe(df_top, use_container_width=True, height=400)
                
#                 # üî• Charts
#                 col1, col2 = st.columns(2)
#                 with col1:
#                     st.subheader("üèôÔ∏è Cities")
#                     if city_counter:
#                         city_df = pd.DataFrame(city_counter.most_common(10), columns=['City', 'Videos'])
#                         fig = px.bar(city_df, x='Videos', y='City', orientation='h', 
#                                    title="Top Cities", color='Videos')
#                         st.plotly_chart(fig, use_container_width=True)
                
#                 with col2:
#                     st.subheader("üåü States")
#                     if state_counter:
#                         state_df = pd.DataFrame(state_counter.most_common(8), columns=['State', 'Videos'])
#                         fig = px.bar(state_df, x='Videos', y='State', orientation='h',
#                                    title="Top States", color='Videos')
#                         st.plotly_chart(fig, use_container_width=True)
                
#                 # üî• Excel Download
#                 st.subheader("üíæ Download Excel")
#                 excel_data, filename = create_excel_bytes(all_videos, analyzed_videos, city_counter, state_counter, query)
#                 st.download_button(
#                     label=f"üì• Download {filename} (5 Sheets)",
#                     data=excel_data,
#                     file_name=filename,
#                     mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
#                 )
                
#                 # üî• Raw Data
#                 with st.expander("üìã All Raw Data"):
#                     st.dataframe(pd.DataFrame(all_videos))
                
#             else:
#                 st.warning("‚ö†Ô∏è No videos found. Try broader keywords like 'skincare'")
#     else:
#         st.error("‚ùå API test failed. Check your key.")

# # üî• Instructions
# with st.expander("üìñ How to get API Key (2 mins)"):
#     st.markdown("""
#     1. Go to [console.cloud.google.com](https://console.cloud.google.com)
#     2. **New Project** ‚Üí Name it
#     3. Search **YouTube Data API v3** ‚Üí **ENABLE**
#     4. **Credentials** ‚Üí **+ CREATE CREDENTIALS** ‚Üí **API Key**
#     5. **Copy 60-char key** ‚Üí Paste in sidebar
#     6. **Test** ‚Üí ‚úÖ Green = Ready!
#     """)

# st.sidebar.markdown("---")
# st.sidebar.markdown("**‚úÖ v26.0 - Battle Tested**\n*Works everywhere*")



# ////////////////////////////////////////////////////////////////////////////////////////////////////


# import streamlit as st
# import requests
# import json
# import re
# import time
# from collections import Counter
# from datetime import datetime, timedelta
# from urllib.parse import quote
# import pandas as pd
# import plotly.express as px
# import io
# import re

# # üî• Safe openpyxl import
# try:
#     import openpyxl
#     EXCEL_AVAILABLE = True
# except ImportError:
#     EXCEL_AVAILABLE = False

# # Page config
# st.set_page_config(page_title="YouTube City Analyzer", layout="wide", page_icon="üì∫")

# BRAND_KEYWORDS = ['loreal', 'maybelline', 'lakme', 'mamaearth', 'nykaa', 'plum']

# INDIA_CITIES = {
#     'kanpur': 'Uttar Pradesh', 'lucknow': 'Uttar Pradesh', 'noida': 'Uttar Pradesh', 
#     'agra': 'Uttar Pradesh', 'varanasi': 'Uttar Pradesh', 'allahabad': 'Uttar Pradesh',
#     'ghaziabad': 'Uttar Pradesh', 'meerut': 'Uttar Pradesh', 'bareilly': 'Uttar Pradesh',
#     'mumbai': 'Maharashtra', 'pune': 'Maharashtra', 'nagpur': 'Maharashtra',
#     'bangalore': 'Karnataka', 'mysore': 'Karnataka', 'delhi': 'Delhi',
#     'chennai': 'Tamil Nadu', 'hyderabad': 'Telangana', 'kolkata': 'West Bengal',
#     'ahmedabad': 'Gujarat', 'jaipur': 'Rajasthan', 'kochi': 'Kerala'
# }

# def safe_api_call(url, api_key, retries=3):
#     """üî• Ultra-safe API call"""
#     for attempt in range(retries):
#         try:
#             response = requests.get(url, timeout=20)
#             if response.status_code == 200:
#                 data = response.json()
#                 if 'error' not in data:
#                     return data
#             elif response.status_code == 429:
#                 time.sleep(10)
#                 continue
#             time.sleep(2)
#         except Exception as e:
#             print(f"Request Error: {e}")
#             time.sleep(2)
#     return None

# def test_api_key(api_key):
#     url = f"https://youtube.googleapis.com/youtube/v3/search?q=test&maxResults=1&key={api_key}"
#     data = safe_api_call(url, api_key)
#     return data is not None

# def extract_hooks_hashtags_keywords(text):
#     """üî• Extract hooks, hashtags, keywords from title+description"""
#     text_lower = text.lower()
    
#     # üî• TOP HOOKS (first 10 words of title - attention grabbers)
#     title_words = re.findall(r'\b\w+\b', text[:200])
#     hooks = title_words[:10]
    
#     # üî• HASHTAGS (#hashtags)
#     hashtags = re.findall(r'#\w+', text)
    
#     # üî• KEYWORDS (important words excluding common ones)
#     common_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'use', 'way'}
#     words = [w for w in re.findall(r'\b\w{3,}\b', text_lower) if w not in common_words and len(w) > 2]
    
#     return hooks, hashtags, words

# def search_videos(query, api_key, max_results=20):
#     """üî• Search ALL video orders to get MAXIMUM results"""
#     video_ids = set()
#     orders = ['date', 'viewCount', 'rating', 'relevance']
    
#     for order in orders:
#         url = f"https://youtube.googleapis.com/youtube/v3/search?part=snippet&q={quote(query)}&type=video&maxResults={max_results}&order={order}&key={api_key}"
#         data = safe_api_call(url, api_key)
#         if data and 'items' in data:
#             for item in data['items']:
#                 video_ids.add(item['id']['videoId'])
        
#         url_in = f"https://youtube.googleapis.com/youtube/v3/search?part=snippet&q={quote(query)}&type=video&maxResults={max_results}&regionCode=IN&order={order}&key={api_key}"
#         data_in = safe_api_call(url_in, api_key)
#         if data_in and 'items' in data_in:
#             for item in data_in['items']:
#                 video_ids.add(item['id']['videoId'])
#         time.sleep(1)
    
#     return list(video_ids)[:100]

# def get_video_details(video_ids, api_key):
#     """üî• Get ALL video details + hooks/hashtags/keywords"""
#     all_videos = []
    
#     for i in range(0, len(video_ids), 20):
#         batch = video_ids[i:i+20]
#         url = f"https://youtube.googleapis.com/youtube/v3/videos?part=snippet,statistics,contentDetails&id={','.join(batch)}&key={api_key}"
#         data = safe_api_call(url, api_key)
        
#         if data and 'items' in data:
#             for item in data['items']:
#                 try:
#                     published_date = item['snippet'].get('publishedAt', '')
#                     pub_datetime = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                    
#                     full_text = item['snippet'].get('title', '') + ' ' + item['snippet'].get('description', '')
                    
#                     # üî• EXTRACT hooks, hashtags, keywords
#                     hooks, hashtags, keywords = extract_hooks_hashtags_keywords(full_text)
                    
#                     video = {
#                         'Video_ID': item['id'],
#                         'Title': item['snippet'].get('title', '')[:100],
#                         'Channel': item['snippet'].get('channelTitle', ''),
#                         'Description': item['snippet'].get('description', '')[:300],
#                         'Published': published_date,
#                         'Published_Date': pub_datetime.strftime('%Y-%m-%d'),
#                         'Views': int(item['statistics'].get('viewCount', 0) or 0),
#                         'Likes': int(item['statistics'].get('likeCount', 0) or 0),
#                         'Comments': int(item['statistics'].get('commentCount', 0) or 0),
#                         'Duration': item['contentDetails'].get('duration', 'PT0S'),
#                         'Video_URL': f"https://youtu.be/{item['id']}",
#                         'Hooks': ', '.join(hooks[:5]),
#                         'Hashtags': ', '.join(hashtags[:10]),
#                         'Keywords': ', '.join(keywords[:8]),
#                         'Region': 'Mixed',
#                         'City': 'Other',
#                         'State': 'Other'
#                     }
                    
#                     # Duration
#                     duration_match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', video['Duration'])
#                     if duration_match:
#                         h, m, s = duration_match.groups()
#                         total_sec = (int(h or 0)*3600 + int(m or 0)*60 + int(s or 0))
#                         video['Duration_Formatted'] = f"{total_sec//60}m {total_sec%60:02d}s"
#                     else:
#                         video['Duration_Formatted'] = '0s'
                    
#                     all_videos.append(video)
#                     time.sleep(0.5)
#                 except Exception as e:
#                     print(f"Video parse error: {e}")
#                     continue
    
#     return all_videos

# def detect_locations(videos):
#     """üî• City/State detection"""
#     city_counter = Counter()
#     state_counter = Counter()
    
#     for video in videos:
#         text = (video['Title'] + ' ' + video['Description']).lower()
#         for city, state in INDIA_CITIES.items():
#             if city in text:
#                 video['City'] = city.title()
#                 video['State'] = state
#                 city_counter[video['City']] += 1
#                 state_counter[state] += 1
#                 break
#         else:
#             video['City'] = 'Other'
#             video['State'] = 'Other'
    
#     return videos, city_counter, state_counter

# def get_top_analysis(videos):
#     """üî• Get top hooks, hashtags, keywords, search cities"""
#     all_hooks = []
#     all_hashtags = Counter()
#     all_keywords = Counter()
#     search_cities = Counter()
    
#     for video in videos:
#         # Hooks (first words)
#         if video['Hooks']:
#             all_hooks.extend(video['Hooks'].split(', '))
        
#         # Hashtags
#         if video['Hashtags']:
#             for tag in video['Hashtags'].split(', '):
#                 all_hashtags[tag] += 1
        
#         # Keywords
#         if video['Keywords']:
#             for kw in video['Keywords'].split(', '):
#                 all_keywords[kw] += 1
        
#         # Search cities (from city detection)
#         if video['City'] != 'Other':
#             search_cities[video['City']] += 1
    
#     return {
#         'top_hooks': Counter(all_hooks).most_common(15),
#         'top_hashtags': all_hashtags.most_common(20),
#         'top_keywords': all_keywords.most_common(20),
#         'top_search_cities': search_cities.most_common(10)
#     }

# def create_excel_bytes(worldwide_videos, india_videos, city_counter, state_counter, analysis, query):
#     """üî• Safe Excel creation with new sheets"""
#     if not EXCEL_AVAILABLE:
#         return None, None
    
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = f"{query.upper().replace(' ', '_')}_ANALYSIS_{timestamp}.xlsx"
    
#     output = io.BytesIO()
#     try:
#         with pd.ExcelWriter(output, engine='openpyxl') as writer:
#             pd.DataFrame(worldwide_videos).to_excel(writer, 'ALL_VIDEOS', index=False)
#             top_videos = sorted(worldwide_videos, key=lambda x: x['Views'], reverse=True)[:50]
#             pd.DataFrame(top_videos).to_excel(writer, 'TOP_50_VIDEOS', index=False)
            
#             pd.DataFrame(city_counter.most_common(20), columns=['City', 'Videos']).to_excel(writer, 'CITY_RANKING', index=False)
#             pd.DataFrame(state_counter.most_common(15), columns=['State', 'Videos']).to_excel(writer, 'STATE_RANKING', index=False)
            
#             # üî• NEW SHEETS
#             pd.DataFrame(analysis['top_hooks'], columns=['Hook', 'Count']).to_excel(writer, 'TOP_HOOKS', index=False)
#             pd.DataFrame(analysis['top_hashtags'], columns=['Hashtag', 'Count']).to_excel(writer, 'TOP_HASHTAGS', index=False)
#             pd.DataFrame(analysis['top_keywords'], columns=['Keyword', 'Count']).to_excel(writer, 'TOP_KEYWORDS', index=False)
#             pd.DataFrame(analysis['top_search_cities'], columns=['City', 'Videos']).to_excel(writer, 'TOP_SEARCH_CITIES', index=False)
        
#         output.seek(0)
#         return output.getvalue(), filename
#     except:
#         return None, None

# # üî• MAIN APP
# st.title("üöÄ YouTube City Analyzer v29.0 - ULTIMATE!")
# st.markdown("***‚úÖ ALL videos + Hooks + Hashtags + Keywords + Search Cities***")

# # üî• Sidebar
# st.sidebar.header("üîë API Setup")
# api_key = st.sidebar.text_input("Your YouTube API Key:", type="password", placeholder="AIzaSyC...")
# query = st.sidebar.text_input("üîç Keyword:", value="lip balm")
# max_results = st.sidebar.slider("Max Videos/Query", 15, 50, 25)

# if st.sidebar.button("üß™ Test API Key", type="secondary"):
#     if api_key:
#         if test_api_key(api_key):
#             st.sidebar.success("‚úÖ API KEY PERFECT! üéâ")
#         else:
#             st.sidebar.error("‚ùå API Key failed")

# # üî• ANALYZE BUTTON
# if st.sidebar.button("üöÄ ANALYZE NOW", type="primary", disabled=not api_key):
#     if test_api_key(api_key):
#         with st.spinner("üîÑ Analyzing YouTube data + hooks/hashtags/keywords..."):
#             video_ids = search_videos(query, api_key, max_results)
#             st.info(f"üì° Found {len(video_ids)} unique video IDs")
            
#             all_videos = get_video_details(video_ids, api_key)
            
#             if all_videos:
#                 analyzed_videos, city_counter, state_counter = detect_locations(all_videos)
#                 analysis = get_top_analysis(all_videos)
                
#                 st.success(f"‚úÖ LOADED {len(all_videos)} videos! üéâ")
                
#                 # üî• DASHBOARD
#                 st.header("üìä COMPLETE ANALYSIS")
                
#                 # Metrics
#                 col1, col2, col3, col4 = st.columns(4)
#                 col1.metric("üì∫ Total Videos", len(all_videos))
#                 col2.metric("üëÄ Total Views", f"{sum(v['Views'] for v in all_videos):,}")
#                 col3.metric("üèôÔ∏è Cities Found", len([c for c in city_counter if c != 'Other']))
#                 col4.metric("üè∑Ô∏è Hashtags Found", sum(len(v['Hashtags'].split(', ')) for v in all_videos if v['Hashtags']))
                
#                 # üî• NEW ANALYSIS SECTION
#                 st.markdown("---")
#                 st.subheader("üî• TOP HOOKS (Attention Grabbers)")
#                 hooks_df = pd.DataFrame(analysis['top_hooks'], columns=['Hook', 'Count'])
#                 st.dataframe(hooks_df, use_container_width=True, height=300)
                
#                 col1, col2 = st.columns(2)
#                 with col1:
#                     st.subheader("üè∑Ô∏è TOP HASHTAGS")
#                     hashtags_df = pd.DataFrame(analysis['top_hashtags'], columns=['Hashtag', 'Count'])
#                     st.dataframe(hashtags_df.head(15), use_container_width=True, height=400)
                
#                 with col2:
#                     st.subheader("üí¨ TOP KEYWORDS")
#                     keywords_df = pd.DataFrame(analysis['top_keywords'], columns=['Keyword', 'Count'])
#                     st.dataframe(keywords_df.head(15), use_container_width=True, height=400)
                
#                 col1, col2 = st.columns(2)
#                 with col1:
#                     st.subheader("üîç TOP SEARCH CITIES")
#                     search_cities_df = pd.DataFrame(analysis['top_search_cities'], columns=['City', 'Videos'])
#                     st.dataframe(search_cities_df, use_container_width=True, height=300)
                
#                 # üî• ORIGINAL DASHBOARD
#                 st.markdown("---")
#                 st.subheader("üìä VIDEO DASHBOARD")
                
#                 # Latest & Top Videos
#                 col1, col2 = st.columns(2)
#                 with col1:
#                     st.markdown("### üÜï LATEST VIDEOS")
#                     latest_videos = sorted(all_videos, key=lambda x: x['Published'], reverse=True)[:15]
#                     st.dataframe(pd.DataFrame(latest_videos)[['Title', 'Channel', 'Published_Date', 'Video_URL']], height=400)
                
#                 with col2:
#                     st.markdown("### üî• TOP VIDEOS")
#                     top_videos = sorted(all_videos, key=lambda x: x['Views'], reverse=True)[:15]
#                     st.dataframe(pd.DataFrame(top_videos)[['Title', 'Channel', 'Views', 'Video_URL']], height=400)
                
#                 # Charts
#                 col1, col2 = st.columns(2)
#                 with col1:
#                     st.markdown("### üèôÔ∏è CITIES")
#                     if city_counter['Other'] != len(all_videos):
#                         city_df = pd.DataFrame(city_counter.most_common(10), columns=['City', 'Videos'])
#                         fig = px.bar(city_df, x='Videos', y='City', orientation='h')
#                         st.plotly_chart(fig, use_container_width=True)
                
#                 with col2:
#                     st.markdown("### üåü STATES")
#                     state_df = pd.DataFrame(state_counter.most_common(8), columns=['State', 'Videos'])
#                     if state_df['State'].iloc[0] != 'Other':
#                         fig = px.bar(state_df, x='Videos', y='State', orientation='h')
#                         st.plotly_chart(fig, use_container_width=True)
                
#                 # üî• Tables
#                 st.markdown("---")
#                 col1, col2 = st.columns(2)
#                 with col1:
#                     st.markdown("### üìã TOP 50 VIDEOS")
#                     top_50 = sorted(all_videos, key=lambda x: x['Views'], reverse=True)[:50]
#                     st.dataframe(pd.DataFrame(top_50)[['Title', 'Views', 'Likes', 'Published_Date', 'City']], height=600)
                
#                 with col2:
#                     st.markdown("### üèôÔ∏è CITY RANKING")
#                     st.dataframe(pd.DataFrame(city_counter.most_common(20), columns=['City', 'Videos']), height=400)
                
#                 with st.expander("üìä ALL RAW DATA + Hooks/Hashtags"):
#                     st.dataframe(pd.DataFrame(all_videos), height=800)
                
#                 # üî• Excel Download (9 SHEETS NOW!)
#                 st.markdown("---")
#                 st.subheader("üíæ Download Excel (9 Sheets)")
#                 excel_data, filename = create_excel_bytes(all_videos, all_videos, city_counter, state_counter, analysis, query)
#                 if excel_data:
#                     st.download_button(
#                         label=f"üì• Download {filename}",
#                         data=excel_data,
#                         file_name=filename,
#                         mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
#                     )
#             else:
#                 st.error("‚ùå NO VIDEOS PROCESSED")
#     else:
#         st.error("‚ùå API Key failed")

# # üî• Instructions
# with st.expander("üìñ API Setup"):
#     st.markdown("""
#     1. [Google Cloud Console](https://console.cloud.google.com)
#     2. New Project ‚Üí **YouTube Data API v3** ‚Üí ENABLE
#     3. Credentials ‚Üí **API Key** ‚Üí Copy & Test ‚úÖ
#     """)

# st.sidebar.markdown("---")
# st.sidebar.markdown("**‚úÖ v29.0 - ULTIMATE ANALYSIS**\n*Hooks + Hashtags + Keywords + Cities*")


import streamlit as st
import requests
import json
import re
import time
from collections import Counter
from datetime import datetime
from urllib.parse import quote
import pandas as pd
import plotly.express as px
import io

# üî• Safe openpyxl import
try:
    import openpyxl
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False

# Page config
st.set_page_config(page_title="YouTube City Analyzer v31.0", layout="wide", page_icon="üì∫")

# Constants
BRAND_KEYWORDS = ['loreal', 'maybelline', 'lakme', 'mamaearth', 'nykaa', 'plum']

INDIA_CITIES = {
    'kanpur': 'Uttar Pradesh', 'lucknow': 'Uttar Pradesh', 'noida': 'Uttar Pradesh', 
    'agra': 'Uttar Pradesh', 'varanasi': 'Uttar Pradesh', 'allahabad': 'Uttar Pradesh',
    'ghaziabad': 'Uttar Pradesh', 'meerut': 'Uttar Pradesh', 'bareilly': 'Uttar Pradesh',
    'mumbai': 'Maharashtra', 'pune': 'Maharashtra', 'nagpur': 'Maharashtra',
    'bangalore': 'Karnataka', 'mysore': 'Karnataka', 'delhi': 'Delhi',
    'chennai': 'Tamil Nadu', 'hyderabad': 'Telangana', 'kolkata': 'West Bengal',
    'ahmedabad': 'Gujarat', 'jaipur': 'Rajasthan', 'kochi': 'Kerala'
}

# üî• DATE FILTER: 23-Dec-2024 to 23-Dec-2025
START_DATE = "2024-12-23T00:00:00Z"
END_DATE = "2025-12-23T23:59:59Z"

def safe_api_call(url, retries=3):
    """üî• Ultra-safe API call - NO PRINTS, Streamlit safe"""
    for attempt in range(retries):
        try:
            response = requests.get(url, timeout=25)
            if response.status_code == 200:
                try:
                    data = response.json()
                    if 'error' not in data:
                        return data
                except:
                    pass
            elif response.status_code == 429:
                time.sleep(15)
                continue
            time.sleep(3)
        except:
            time.sleep(3)
    return None

def test_api_key(api_key):
    """üî• FIXED API TEST - Works 100%"""
    if len(api_key) < 35:
        return False
    
    url = f"https://youtube.googleapis.com/youtube/v3/search?q=test&maxResults=1&key={api_key}"
    data = safe_api_call(url)
    return data is not None and isinstance(data, dict)

def search_videos(query, api_key, max_results=30):
    """üî• Search videos with date filter"""
    video_ids = set()
    orders = ['relevance', 'viewCount', 'date', 'rating']
    
    for order in orders:
        # Worldwide search
        url = f"https://youtube.googleapis.com/youtube/v3/search?part=snippet&q={quote(query)}&type=video&maxResults={max_results}&order={order}&publishedAfter={START_DATE}&publishedBefore={END_DATE}&key={api_key}"
        data = safe_api_call(url)
        if data and 'items' in data:
            for item in data['items']:
                if 'id' in item and 'videoId' in item['id']:
                    video_ids.add(item['id']['videoId'])
        
        # India specific
        url_in = f"https://youtube.googleapis.com/youtube/v3/search?part=snippet&q={quote(query)}&type=video&maxResults={max_results}&regionCode=IN&order={order}&publishedAfter={START_DATE}&publishedBefore={END_DATE}&key={api_key}"
        data_in = safe_api_call(url_in)
        if data_in and 'items' in data_in:
            for item in data_in['items']:
                if 'id' in item and 'videoId' in item['id']:
                    video_ids.add(item['id']['videoId'])
        
        time.sleep(2)
    
    return list(video_ids)[:100]

def get_video_details(video_ids, api_key):
    """üî• Get detailed video info with STRICT date filter"""
    all_videos = []
    if not video_ids:
        return all_videos
        
    start_dt = datetime.fromisoformat(START_DATE.replace('Z', '+00:00'))
    end_dt = datetime.fromisoformat(END_DATE.replace('Z', '+00:00'))
    
    for i in range(0, len(video_ids), 50):  # Bigger batches
        batch = video_ids[i:i+50]
        url = f"https://youtube.googleapis.com/youtube/v3/videos?part=snippet,statistics,contentDetails&id={','.join(batch)}&key={api_key}"
        data = safe_api_call(url)
        
        if data and 'items' in data:
            for item in data['items']:
                try:
                    published_date = item['snippet'].get('publishedAt', '')
                    pub_datetime = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                    
                    # üî• STRICT DATE FILTER
                    if not (start_dt <= pub_datetime <= end_dt):
                        continue
                    
                    full_text = f"{item['snippet'].get('title', '')} {item['snippet'].get('description', '')}"
                    hooks, hashtags, keywords = extract_hooks_hashtags_keywords(full_text)
                    
                    video = {
                        'Video_ID': item['id'],
                        'Title': item['snippet'].get('title', '')[:120],
                        'Channel': item['snippet'].get('channelTitle', ''),
                        'Description': item['snippet'].get('description', '')[:400],
                        'Published': published_date,
                        'Published_Date': pub_datetime.strftime('%Y-%m-%d %H:%M'),
                        'Views': int(item['statistics'].get('viewCount', 0) or 0),
                        'Likes': int(item['statistics'].get('likeCount', 0) or 0),
                        'Comments': int(item['statistics'].get('commentCount', 0) or 0),
                        'Duration': item['contentDetails'].get('duration', 'PT0S'),
                        'Video_URL': f"https://youtu.be/{item['id']}",
                        'Hooks': ', '.join(hooks[:6]),
                        'Hashtags': ', '.join(hashtags[:8]),
                        'Keywords': ', '.join(keywords[:8]),
                        'City': 'Other',
                        'State': 'Other'
                    }
                    
                    # Parse duration
                    duration_match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', video['Duration'])
                    if duration_match:
                        h, m, s = duration_match.groups()
                        total_sec = (int(h or 0)*3600 + int(m or 0)*60 + int(s or 0))
                        video['Duration_Formatted'] = f"{total_sec//60}m {total_sec%60:02d}s"
                    else:
                        video['Duration_Formatted'] = 'Live'
                    
                    all_videos.append(video)
                    time.sleep(0.3)
                    
                except Exception:
                    continue
    
    return all_videos

def extract_hooks_hashtags_keywords(text):
    """üî• Extract hooks, hashtags, keywords from text"""
    text_lower = text.lower()
    
    # Hooks (title words)
    title_words = re.findall(r'\b[a-zA-Z]{3,15}\b', text[:250])
    
    # Hashtags
    hashtags = re.findall(r'#\w+', text)
    
    # Keywords (remove common words)
    common_words = {
        'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 
        'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 
        'now', 'old', 'see', 'two', 'use', 'way', 'with', 'this', 'that', 'from', 'have'
    }
    words = [w for w in re.findall(r'\b[a-zA-Z]{3,12}\b', text_lower) 
             if w not in common_words and len(w) > 2]
    
    return title_words[:8], hashtags[:10], words[:10]

def detect_locations(videos):
    """üî• Detect Indian cities in video content"""
    city_counter = Counter()
    state_counter = Counter()
    
    for video in videos:
        text = (video['Title'] + ' ' + video['Description']).lower()
        for city_key, state in INDIA_CITIES.items():
            if city_key in text:
                video['City'] = city_key.title()
                video['State'] = state
                city_counter[video['City']] += 1
                state_counter[state] += 1
                break
    
    return videos, city_counter, state_counter

def get_top_analysis(videos):
    """üî• Generate analysis data"""
    all_hooks = []
    all_hashtags = Counter()
    all_keywords = Counter()
    search_cities = Counter()
    
    for video in videos:
        # Hooks
        if video.get('Hooks'):
            all_hooks.extend([h.strip() for h in video['Hooks'].split(',') if h.strip()])
        
        # Hashtags
        if video.get('Hashtags'):
            all_hashtags.update([tag.strip() for tag in video['Hashtags'].split(',') if tag.strip()])
        
        # Keywords
        if video.get('Keywords'):
            all_keywords.update([kw.strip() for kw in video['Keywords'].split(',') if kw.strip()])
        
        # Cities
        if video.get('City', 'Other') != 'Other':
            search_cities[video['City']] += 1
    
    return {
        'top_hooks': Counter(all_hooks).most_common(12),
        'top_hashtags': all_hashtags.most_common(18),
        'top_keywords': all_keywords.most_common(18),
        'top_search_cities': search_cities.most_common(12)
    }

def create_excel_bytes(videos, city_counter, state_counter, analysis, query):
    """üî• Create Excel file in memory"""
    if not EXCEL_AVAILABLE:
        return None, None
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"YOUTUBE_{query.upper().replace(' ', '_')}_20241223_20251223_{timestamp}.xlsx"
    
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # All videos
            pd.DataFrame(videos).to_excel(writer, 'ALL_VIDEOS_2024-2025', index=False)
            
            # Top videos
            top_videos = sorted(videos, key=lambda x: x['Views'], reverse=True)[:50]
            pd.DataFrame(top_videos).to_excel(writer, 'TOP_50_VIDEOS', index=False)
            
            # Cities
            pd.DataFrame(city_counter.most_common(20), columns=['City', 'Videos']).to_excel(writer, 'CITY_RANKING', index=False)
            
            # States
            pd.DataFrame(state_counter.most_common(15), columns=['State', 'Videos']).to_excel(writer, 'STATE_RANKING', index=False)
            
            # Analysis
            pd.DataFrame(analysis['top_hooks'], columns=['Hook', 'Count']).to_excel(writer, 'TOP_HOOKS', index=False)
            pd.DataFrame(analysis['top_hashtags'], columns=['Hashtag', 'Count']).to_excel(writer, 'TOP_HASHTAGS', index=False)
            pd.DataFrame(analysis['top_keywords'], columns=['Keyword', 'Count']).to_excel(writer, 'TOP_KEYWORDS', index=False)
            
        output.seek(0)
        return output.getvalue(), filename
    except:
        return None, None

# üî• MAIN APP
st.title("üöÄ YouTube City Analyzer v31.0 - 2024-2025 DATA ONLY!")
st.markdown("***‚úÖ 23-Dec-2024 ‡§∏‡•á 23-Dec-2025 | Hooks + Hashtags + Cities + Excel Export***")

# üî• Sidebar
st.sidebar.header("üîß Setup")
api_key = st.sidebar.text_input("YouTube API Key:", type="password", placeholder="AIzaSyC... (39+ chars)")
query = st.sidebar.text_input("üîç Keyword:", value="lip balm")
max_results = st.sidebar.slider("Max Videos per Query:", 20, 50, 30)

st.sidebar.markdown("---")
st.sidebar.info(f"üìÖ **Date Filter**: 23-Dec-2024 ‡§∏‡•á 23-Dec-2025")

# üî• API TEST BUTTON
if st.sidebar.button("üß™ Test API Key", type="secondary"):
    if not api_key:
        st.sidebar.warning("üëà Enter API key first!")
    elif len(api_key) < 35:
        st.sidebar.error("‚ùå Key too short! Need 39+ characters")
    elif test_api_key(api_key):
        st.sidebar.success("‚úÖ API KEY PERFECT! üéâ")
        st.sidebar.balloons()
    else:
        st.sidebar.error("‚ùå API Key failed!")
        st.sidebar.info("""
        **üîß Quick Fix (2 mins):**
        1. [Google Cloud Console](https://console.cloud.google.com)
        2. New Project ‚Üí "YouTube2025"
        3. APIs ‚Üí "YouTube Data API v3" ‚Üí **ENABLE**
        4. Credentials ‚Üí **+ CREATE CREDENTIALS** ‚Üí API Key
        5. Copy FULL key ‚Üí Test again ‚úÖ
        """)

# üî• ANALYZE BUTTON
if st.sidebar.button("üöÄ ANALYZE NOW", type="primary", disabled=not api_key):
    if test_api_key(api_key):
        with st.spinner("üîÑ Fetching 2024-2025 YouTube data..."):
            st.info(f"üîç **Query**: '{query}' | üìÖ **Date**: 23-Dec-2024 ‡§∏‡•á 23-Dec-2025")
            
            # Search videos
            video_ids = search_videos(query, api_key, max_results)
            st.success(f"üì° Found **{len(video_ids)}** video IDs!")
            
            # Get details
            all_videos = get_video_details(video_ids, api_key)
            
            if all_videos:
                # Analyze
                analyzed_videos, city_counter, state_counter = detect_locations(all_videos)
                analysis = get_top_analysis(all_videos)
                
                st.success(f"‚úÖ **{len(all_videos)} videos** analyzed from 2024-2025! üéâ")
                
                # üî• DASHBOARD
                st.markdown("---")
                st.header("üìä 2024-2025 ANALYSIS DASHBOARD")
                
                # Metrics
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("üì∫ Total Videos", len(all_videos))
                col2.metric("üëÄ Total Views", f"{sum(v['Views'] for v in all_videos):,}")
                col3.metric("‚ù§Ô∏è Total Likes", f"{sum(v['Likes'] for v in all_videos):,}")
                col4.metric("üèôÔ∏è Cities Found", len([c for c in city_counter if c != 'Other']))
                
                # üî• Videos Tables
                st.markdown("---")
                col1, col2 = st.columns(2)
                with col1:
                    st.subheader("üÜï Latest Videos (2024-25)")
                    latest = sorted(all_videos, key=lambda x: x['Published'], reverse=True)[:15]
                    st.dataframe(
                        pd.DataFrame(latest)[['Title', 'Published_Date', 'Views', 'Video_URL']], 
                        use_container_width=True, height=450
                    )
                
                with col2:
                    st.subheader("üî• Top Videos by Views")
                    top_videos = sorted(all_videos, key=lambda x: x['Views'], reverse=True)[:15]
                    st.dataframe(
                        pd.DataFrame(top_videos)[['Title', 'Views', 'Likes', 'Video_URL']], 
                        use_container_width=True, height=450
                    )
                
                # üî• Analysis Charts
                st.markdown("---")
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("üèôÔ∏è City Distribution")
                    if city_counter['Other'] != len(all_videos):
                        city_df = pd.DataFrame(city_counter.most_common(12), columns=['City', 'Videos'])
                        fig_city = px.bar(city_df, x='Videos', y='City', orientation='h',
                                        color='Videos', color_continuous_scale='Viridis')
                        st.plotly_chart(fig_city, use_container_width=True)
                
                with col2:
                    st.subheader("üè∑Ô∏è Top Hashtags")
                    hashtag_df = pd.DataFrame(analysis['top_hashtags'][:12], columns=['Hashtag', 'Count'])
                    st.dataframe(hashtag_df, use_container_width=True, height=350)
                
                # üî• More Analysis
                col3, col4 = st.columns(2)
                with col3:
                    st.subheader("üî• Top Hooks")
                    hooks_df = pd.DataFrame(analysis['top_hooks'], columns=['Hook', 'Count'])
                    st.dataframe(hooks_df, use_container_width=True, height=300)
                
                with col4:
                    st.subheader("üí¨ Top Keywords")
                    keywords_df = pd.DataFrame(analysis['top_keywords'][:12], columns=['Keyword', 'Count'])
                    st.dataframe(keywords_df, use_container_width=True, height=300)
                
                # üî• Excel Download
                st.markdown("---")
                st.subheader("üíæ Download Full Report")
                excel_data, filename = create_excel_bytes(all_videos, city_counter, state_counter, analysis, query)
                if excel_data:
                    st.download_button(
                        label=f"üì• Download Excel Report ({filename})",
                        data=excel_data,
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                else:
                    st.info("üìä **Excel unavailable** - All data shown above!")
            else:
                st.warning("‚ö†Ô∏è **No videos found** in date range 23-Dec-2024 ‡§∏‡•á 23-Dec-2025")
                st.info("üí° **Try these keywords**:")
                st.markdown("- `skincare`")
                st.markdown("- `lipstick`") 
                st.markdown("- `mamaearth`")
                st.markdown("- `hair oil`")
    else:
        st.error("‚ùå **API Key failed!** Test first ‚ûú üß™ Test API Key")

# üî• Instructions Expander
with st.expander("üìñ Complete API Setup Guide (2 Minutes)"):
    st.markdown("""
    ### **Step-by-Step API Key Setup:**
    
    1. **Go to**: [console.cloud.google.com](https://console.cloud.google.com)
    2. **NEW PROJECT** ‚Üí Name: "YouTubeAnalyzer2025"
    3. **APIs & Services** ‚Üí **Library**
    4. Search: **"YouTube Data API v3"** ‚Üí **ENABLE** (Blue button)
    5. **Credentials** ‚Üí **+ CREATE CREDENTIALS** ‚Üí **API Key**
    6. **COPY FULL KEY** (39+ characters) ‚Üí Paste in sidebar
    7. **üß™ Test API Key** ‚Üí **‚úÖ GREEN SUCCESS**
    
    **Daily Limit**: 10,000 requests (FREE)
    """)

st.markdown("---")
st.markdown("*‚úÖ v31.0 - Production Ready | No Crash | API Fixed | 2024-2025 Data Only*")
