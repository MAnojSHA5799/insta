import streamlit as st
import requests
import json
import re
import time
from collections import Counter
from datetime import datetime, timedelta
from urllib.parse import quote
import pandas as pd
import plotly.express as px
import io

# ğŸ”¥ Safe openpyxl import
try:
    import openpyxl
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False

# Page config
st.set_page_config(page_title="YouTube City Analyzer", layout="wide", page_icon="ğŸ“º")

BRAND_KEYWORDS = ['loreal', 'maybelline', 'lakme', 'mamaearth', 'nykaa', 'plum']

INDIA_CITIES = {
    'kanpur': 'Uttar Pradesh', 'lucknow': 'Uttar Pradesh', 'noida': 'Uttar Pradesh', 
    'agra': 'Uttar Pradesh', 'varanasi': 'Uttar Pradesh', 'allahabad': 'Uttar Pradesh',
    'ghaziabad': 'Uttar Pradesh', 'meerut': 'Uttar Pradesh', 'bareilly': 'Uttar Pradesh',
    'mumbai': 'Maharashtra', 'pune': 'Maharashtra', 'nagpur': 'Maharashtra',
    'bangalore': 'Karnataka', 'mysore': 'Karnataka', 'delhi': 'Delhi',
    'chennai': 'Tamil Nadu', 'hyderabad': 'Telangana', 'kolkata': 'West Bengal',
    'ahmedabad': 'Gujarat', 'jaipur': 'Rajasthan', 'kochi': 'Kerala'
}

def safe_api_call(url, api_key, retries=3):
    """ğŸ”¥ Ultra-safe API call"""
    for attempt in range(retries):
        try:
            response = requests.get(url, timeout=20)
            if response.status_code == 200:
                data = response.json()
                if 'error' not in data:
                    return data
            elif response.status_code == 429:
                time.sleep(10)
                continue
            time.sleep(2)
        except Exception as e:
            print(f"Request Error: {e}")
            time.sleep(2)
    return None

def test_api_key(api_key):
    url = f"https://youtube.googleapis.com/youtube/v3/search?q=test&maxResults=1&key={api_key}"
    data = safe_api_call(url, api_key)
    return data is not None

def search_videos(query, api_key, max_results=20):
    """ğŸ”¥ Search ALL video orders to get MAXIMUM results"""
    video_ids = set()
    
    # ğŸ”¥ TRY ALL SORT ORDERS to get more videos
    orders = ['date', 'viewCount', 'rating', 'relevance']
    
    for order in orders:
        # Worldwide
        url = f"https://youtube.googleapis.com/youtube/v3/search?part=snippet&q={quote(query)}&type=video&maxResults={max_results}&order={order}&key={api_key}"
        data = safe_api_call(url, api_key)
        if data and 'items' in data:
            for item in data['items']:
                video_ids.add(item['id']['videoId'])
        
        # India
        url_in = f"https://youtube.googleapis.com/youtube/v3/search?part=snippet&q={quote(query)}&type=video&maxResults={max_results}&regionCode=IN&order={order}&key={api_key}"
        data_in = safe_api_call(url_in, api_key)
        if data_in and 'items' in data_in:
            for item in data_in['items']:
                video_ids.add(item['id']['videoId'])
        
        time.sleep(1)
    
    return list(video_ids)[:100]  # ğŸ”¥ MORE VIDEO IDs

def get_video_details(video_ids, api_key):
    """ğŸ”¥ NO DATE FILTER - SHOW ALL VIDEOS"""
    all_videos = []
    
    for i in range(0, len(video_ids), 20):
        batch = video_ids[i:i+20]
        url = f"https://youtube.googleapis.com/youtube/v3/videos?part=snippet,statistics,contentDetails&id={','.join(batch)}&key={api_key}"
        data = safe_api_call(url, api_key)
        
        if data and 'items' in data:
            for item in data['items']:
                try:
                    published_date = item['snippet'].get('publishedAt', '')
                    pub_datetime = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                    
                    video = {
                        'Video_ID': item['id'],
                        'Title': item['snippet'].get('title', '')[:100],
                        'Channel': item['snippet'].get('channelTitle', ''),
                        'Description': item['snippet'].get('description', '')[:300],
                        'Published': published_date,
                        'Published_Date': pub_datetime.strftime('%Y-%m-%d'),
                        'Views': int(item['statistics'].get('viewCount', 0) or 0),
                        'Likes': int(item['statistics'].get('likeCount', 0) or 0),
                        'Comments': int(item['statistics'].get('commentCount', 0) or 0),
                        'Duration': item['contentDetails'].get('duration', 'PT0S'),
                        'Video_URL': f"https://youtu.be/{item['id']}",
                        'Region': 'Mixed',
                        'City': 'Other',
                        'State': 'Other'
                    }
                    
                    # Duration
                    duration_match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', video['Duration'])
                    if duration_match:
                        h, m, s = duration_match.groups()
                        total_sec = (int(h or 0)*3600 + int(m or 0)*60 + int(s or 0))
                        video['Duration_Formatted'] = f"{total_sec//60}m {total_sec%60:02d}s"
                    else:
                        video['Duration_Formatted'] = '0s'
                    
                    all_videos.append(video)
                    time.sleep(0.5)
                except Exception as e:
                    print(f"Video parse error: {e}")
                    continue
    
    return all_videos

def detect_locations(videos):
    """ğŸ”¥ City/State detection"""
    city_counter = Counter()
    state_counter = Counter()
    
    for video in videos:
        text = (video['Title'] + ' ' + video['Description']).lower()
        for city, state in INDIA_CITIES.items():
            if city in text:
                video['City'] = city.title()
                video['State'] = state
                city_counter[video['City']] += 1
                state_counter[state] += 1
                break
        else:
            video['City'] = 'Other'
            video['State'] = 'Other'
    
    return videos, city_counter, state_counter

def create_excel_bytes(worldwide_videos, india_videos, city_counter, state_counter, query):
    """ğŸ”¥ Safe Excel creation"""
    if not EXCEL_AVAILABLE:
        return None, None
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{query.upper().replace(' ', '_')}_ANALYSIS_{timestamp}.xlsx"
    
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            pd.DataFrame(worldwide_videos).to_excel(writer, 'ALL_VIDEOS', index=False)
            top_all = sorted(worldwide_videos, key=lambda x: x['Views'], reverse=True)[:50]
            pd.DataFrame(top_all).to_excel(writer, 'TOP_50_VIDEOS', index=False)
            city_df = pd.DataFrame(city_counter.most_common(20), columns=['City', 'Videos'])
            city_df.to_excel(writer, 'CITY_RANKING', index=False)
            state_df = pd.DataFrame(state_counter.most_common(15), columns=['State', 'Videos'])
            state_df.to_excel(writer, 'STATE_RANKING', index=False)
        output.seek(0)
        return output.getvalue(), filename
    except:
        return None, None

# ğŸ”¥ MAIN APP
st.title("ğŸš€ YouTube City Analyzer v28.0 - MAXIMUM DATA!")
st.markdown("***âœ… ALL videos | No date filter | 4x search methods | 100+ videos***")

# ğŸ”¥ Sidebar
st.sidebar.header("ğŸ”‘ API Setup")
api_key = st.sidebar.text_input("Your YouTube API Key:", type="password", 
                               placeholder="AIzaSyC... (60 characters)")

query = st.sidebar.text_input("ğŸ” Keyword:", value="lip balm")
max_results = st.sidebar.slider("Max Videos/Query", 15, 50, 25)

if st.sidebar.button("ğŸ§ª Test API Key", type="secondary"):
    if api_key:
        if test_api_key(api_key):
            st.sidebar.success("âœ… API KEY PERFECT! ğŸ‰")
        else:
            st.sidebar.error("âŒ API Key failed")

# ğŸ”¥ ANALYZE BUTTON
if st.sidebar.button("ğŸš€ ANALYZE NOW", type="primary", disabled=not api_key):
    if test_api_key(api_key):
        with st.spinner("ğŸ”„ Fetching MAXIMUM YouTube data..."):
            st.info("ğŸ” Using 4 search methods: date, views, rating, relevance")
            
            video_ids = search_videos(query, api_key, max_results)
            st.info(f"ğŸ“¡ Found {len(video_ids)} unique video IDs")
            
            all_videos = get_video_details(video_ids, api_key)
            
            if all_videos:
                analyzed_videos, city_counter, state_counter = detect_locations(all_videos)
                
                st.success(f"âœ… LOADED {len(all_videos)} videos! ğŸ‰")
                
                # ğŸ”¥ DASHBOARD
                st.header("ğŸ“Š COMPLETE RESULTS")
                
                # Metrics
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("ğŸ“º Total Videos", len(all_videos))
                col2.metric("ğŸ‘€ Total Views", f"{sum(v['Views'] for v in all_videos):,}")
                col3.metric("ğŸ™ï¸ Cities Found", len([c for c in city_counter if c != 'Other']))
                col4.metric("â¤ï¸ Total Likes", f"{sum(v['Likes'] for v in all_videos):,}")
                
                # ğŸ”¥ LATEST VIDEOS
                st.subheader("ğŸ†• LATEST VIDEOS")
                latest_videos = sorted(all_videos, key=lambda x: x['Published'], reverse=True)[:20]
                df_latest = pd.DataFrame(latest_videos)[['Title', 'Channel', 'Published_Date', 'Views', 'Video_URL']]
                st.dataframe(df_latest, use_container_width=True, height=400)
                
                # ğŸ”¥ TOP VIDEOS
                st.subheader("ğŸ”¥ TOP VIDEOS by Views")
                top_videos = sorted(all_videos, key=lambda x: x['Views'], reverse=True)[:20]
                df_top = pd.DataFrame(top_videos)[['Title', 'Channel', 'Views', 'Likes', 'Video_URL']]
                st.dataframe(df_top, use_container_width=True, height=400)
                
                # ğŸ”¥ Charts
                col1, col2 = st.columns(2)
                with col1:
                    st.subheader("ğŸ™ï¸ Cities")
                    if city_counter['Other'] != len(all_videos):
                        city_df = pd.DataFrame(city_counter.most_common(10), columns=['City', 'Videos'])
                        fig = px.bar(city_df, x='Videos', y='City', orientation='h', title="Top Cities")
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("No Indian cities detected")
                
                with col2:
                    st.subheader("ğŸŒŸ States")
                    state_df = pd.DataFrame(state_counter.most_common(8), columns=['State', 'Videos'])
                    if state_df['State'].iloc[0] != 'Other':
                        fig = px.bar(state_df, x='Videos', y='State', orientation='h', title="Top States")
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("No Indian states detected")
                
                # ğŸ”¥ ALL TABLES
                st.markdown("---")
                
                st.subheader("ğŸ“‹ TOP 50 VIDEOS")
                top_50 = sorted(all_videos, key=lambda x: x['Views'], reverse=True)[:50]
                df_top50 = pd.DataFrame(top_50)[['Title', 'Channel', 'Views', 'Likes', 'Published_Date', 'City', 'Video_URL']]
                st.dataframe(df_top50, use_container_width=True, height=600)
                
                st.subheader("ğŸ™ï¸ CITY RANKING")
                city_df_full = pd.DataFrame(city_counter.most_common(20), columns=['City', 'Videos'])
                st.dataframe(city_df_full, use_container_width=True, height=300)
                
                st.subheader("ğŸŒŸ STATE RANKING")
                state_df_full = pd.DataFrame(state_counter.most_common(15), columns=['State', 'Videos'])
                st.dataframe(state_df_full, use_container_width=True, height=300)
                
                with st.expander("ğŸ“Š ALL RAW DATA"):
                    st.dataframe(pd.DataFrame(all_videos), use_container_width=True, height=800)
                
                # ğŸ”¥ Excel
                st.subheader("ğŸ’¾ Download Excel")
                excel_data, filename = create_excel_bytes(all_videos, all_videos, city_counter, state_counter, query)
                if excel_data:
                    st.download_button(
                        label=f"ğŸ“¥ Download {filename}",
                        data=excel_data,
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                else:
                    st.info("ğŸ“Š Excel unavailable - All data shown above!")
            else:
                st.error("âŒ NO VIDEOS PROCESSED. Check API quota!")
                st.info(f"ğŸ” Searched: '{query}' | ğŸ“¡ Got {len(video_ids)} IDs")
    else:
        st.error("âŒ API Key failed")

# ğŸ”¥ Instructions
with st.expander("ğŸ“– API Setup"):
    st.markdown("""
    1. [Google Cloud Console](https://console.cloud.google.com)
    2. New Project â†’ **YouTube Data API v3** â†’ ENABLE
    3. Credentials â†’ **API Key**
    4. Copy key â†’ Test âœ…
    """)

st.sidebar.markdown("---")
st.sidebar.markdown("**âœ… v28.0 - MAX DATA**\n*No date filter | 100+ videos*")
